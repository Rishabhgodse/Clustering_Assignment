{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdvSNzquVDdQ"
      },
      "outputs": [],
      "source": [
        "# 1. What is unsupervised learning in the context of machine learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsupervised learning is a type of machine learning where the model is trained on data without labeled responses. The goal is to identify patterns, groupings, or structure within the data. Common tasks include clustering and dimensionality reduction."
      ],
      "metadata": {
        "id": "Sl-ZFs0vVTpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. How does K-Means clustering algorithm work?"
      ],
      "metadata": {
        "id": "8zngbTrZVWKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means works by partitioning data into K clusters. It initializes K centroids, assigns each data point to the nearest centroid, recalculates the centroids as the mean of the points in each cluster, and repeats the process until the centroids stabilize (convergence)."
      ],
      "metadata": {
        "id": "JGUyIhASVZ_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Explain the concept of a dendrogram in hierarchical clustering."
      ],
      "metadata": {
        "id": "p_LMwPEJVcky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dendrogram is a tree-like diagram that records the sequences of merges or splits in hierarchical clustering. It shows how clusters are formed and helps decide the optimal number of clusters by cutting the tree at a desired height."
      ],
      "metadata": {
        "id": "eHOsNq0gVmu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. What is the main difference between K-Means and Hierarchical Clustering?"
      ],
      "metadata": {
        "id": "t0-U4sJoVpAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means requires the number of clusters to be predefined and creates non-overlapping clusters, whereas hierarchical clustering builds a hierarchy of clusters and does not require the number of clusters initially."
      ],
      "metadata": {
        "id": "tgcuY0xEVtBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. What are the advantages of DBSCAN over K-Means?"
      ],
      "metadata": {
        "id": "47TCuexFV3_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No need to predefine the number of clusters\n",
        "\n",
        "Can detect clusters of arbitrary shape\n",
        "\n",
        "Can identify and label noise/outliers\n",
        "\n",
        "More robust to clusters with varying densities"
      ],
      "metadata": {
        "id": "I5AKWIPxV5-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. When would you use Silhouette Score in clustering?"
      ],
      "metadata": {
        "id": "VRO-Qb2AV9RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette Score is used to evaluate the quality of clusters. It measures how similar a point is to its own cluster compared to other clusters. A high score indicates well-separated and cohesive clusters."
      ],
      "metadata": {
        "id": "7iWcMImsWBhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. What are the limitations of Hierarchical Clustering?"
      ],
      "metadata": {
        "id": "wzhVrb0xWEXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scalability: Not efficient for large datasets\n",
        "\n",
        "No re-evaluation: Once a merge or split is done, it can't be undone\n",
        "\n",
        "Sensitive to noisy data and outliers"
      ],
      "metadata": {
        "id": "3yt314MxWH5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Why is feature scaling important in clustering algorithms like K-Means?"
      ],
      "metadata": {
        "id": "STsMKgSJWKSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means uses distance-based metrics (like Euclidean distance), which can be biased if features have different units or scales. Scaling ensures all features contribute equally to the distance calculations."
      ],
      "metadata": {
        "id": "QNWgJs9cWOhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. How does DBSCAN identify noise points?"
      ],
      "metadata": {
        "id": "J05goY3uWRms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN labels a point as noise if it doesn't have enough neighbors (less than minPts) within a given distance (eps) and is not part of any dense region (cluster)."
      ],
      "metadata": {
        "id": "o95dIDVqWX_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Define inertia in the context of K-Means?"
      ],
      "metadata": {
        "id": "J8kMOE-CWU7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inertia is the sum of squared distances between each data point and its assigned centroid. Lower inertia indicates more compact clusters, but it may decrease with more clusters, hence it should be balanced with model complexity."
      ],
      "metadata": {
        "id": "n-MIovcgWgpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. What is the elbow method in K-Means clustering?"
      ],
      "metadata": {
        "id": "PWOduAE_WjpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The elbow method helps determine the optimal number of clusters by plotting inertia vs. the number of clusters. The \"elbow point\" (where the curve bends) is considered the best trade-off between complexity and performance."
      ],
      "metadata": {
        "id": "iC0nKgTCWp5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Describe the concept of \"density\" in DBSCAN."
      ],
      "metadata": {
        "id": "nKKXYl6CWqs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In DBSCAN, density refers to the number of data points within a certain radius (eps). If a point has enough neighboring points (≥ minPts), it is considered part of a dense region or a cluster."
      ],
      "metadata": {
        "id": "yElRWv7CWvm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Can hierarchical clustering be used on categorical data?"
      ],
      "metadata": {
        "id": "U0KFen_OWzGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, but standard hierarchical clustering methods rely on distance metrics, so categorical data needs to be encoded (e.g., one-hot encoding), or distance measures like Hamming distance can be used."
      ],
      "metadata": {
        "id": "oQtH9pM8W3ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. What does a negative Silhouette Score indicate?"
      ],
      "metadata": {
        "id": "WsiDwxHdW6ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A negative Silhouette Score means that a data point is closer to a different cluster than the one it was assigned to. This suggests poor clustering quality or overlapping clusters."
      ],
      "metadata": {
        "id": "9BMWEuYXXDP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Explain the term \"linkage criteria\" in hierarchical clustering?"
      ],
      "metadata": {
        "id": "3lh2rBayXG4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linkage criteria determine how the distance between clusters is calculated when merging them. Common types include:\n",
        "\n",
        "Single linkage: shortest distance\n",
        "\n",
        "Complete linkage: longest distance\n",
        "\n",
        "Average linkage: average distance\n",
        "\n",
        "Ward’s method: minimizes total within-cluster variance"
      ],
      "metadata": {
        "id": "oBEmTHSJXNR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Why might K-Means clustering perform poorly on data with varying cluster sizes or densities?"
      ],
      "metadata": {
        "id": "ctHUKHI0XQP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means assumes equal-sized, spherical clusters and is sensitive to outliers. It performs poorly when clusters have:\n",
        "\n",
        "Different sizes or densities\n",
        "\n",
        "Non-spherical shapes\n",
        "\n",
        "Presence of noise or outliers"
      ],
      "metadata": {
        "id": "7WBq4HoEXUt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. What are the core parameters in DBSCAN, and how do they influence clustering?"
      ],
      "metadata": {
        "id": "HDnHze74XXO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "eps (epsilon): the maximum distance between two samples to be considered neighbors\n",
        "\n",
        "minPts: the minimum number of points required to form a dense region\n",
        "These control cluster compactness and minimum size, affecting the number and shape of detected clusters."
      ],
      "metadata": {
        "id": "rqNX1UbTXal2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. How does K-Means++ improve upon standard K-Means initialization?"
      ],
      "metadata": {
        "id": "VkLFlc5CXc52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means++ selects initial centroids in a smart way that spreads them out, reducing the chances of poor clustering and increasing convergence speed. It improves cluster quality and avoids local minima."
      ],
      "metadata": {
        "id": "MoWbTMurXgqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. What is agglomerative clustering?"
      ],
      "metadata": {
        "id": "6lUJLhuyXjea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agglomerative clustering is a type of hierarchical clustering that starts with each data point as an individual cluster and merges the closest clusters step by step until only one cluster remains or a stopping criterion is met."
      ],
      "metadata": {
        "id": "jRDKYF4aXnZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. What makes Silhouette Score a better metric than just inertia for model evaluation?"
      ],
      "metadata": {
        "id": "_bKPx2D_XqSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inertia only considers intra-cluster distance, not how well-separated clusters are. Silhouette Score evaluates both:\n",
        "\n",
        "Cohesion (intra-cluster)\n",
        "\n",
        "Separation (inter-cluster) This makes it a more balanced and interpretable metric for assessing clustering performance."
      ],
      "metadata": {
        "id": "LHRcjbDHXt6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                        # Practical Questions"
      ],
      "metadata": {
        "id": "kr1vK9jMXwi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "y_kmeans = kmeans.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='red', marker='X')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K-VTi17PrF9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 22. Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X)\n",
        "print(labels[:10])\n"
      ],
      "metadata": {
        "id": "eRVXwqztrLFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 23. Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X, y = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "db = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
        "labels = db.labels_\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Paired', s=50)\n",
        "plt.scatter(X[labels == -1, 0], X[labels == -1, 1], color='red', label='Outliers')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9usAJvvJrLpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X = StandardScaler().fit_transform(wine.data)\n",
        "km = KMeans(n_clusters=3, random_state=42)\n",
        "labels = km.fit_predict(X)\n",
        "\n",
        "import numpy as np\n",
        "print(np.bincount(labels))\n"
      ],
      "metadata": {
        "id": "vetRteMurMOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 25. Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, y = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "db = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
        "labels = db.labels_\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='Paired')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tAOnoyCVrfEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 26. Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = MinMaxScaler().fit_transform(data.data)\n",
        "km = KMeans(n_clusters=2, random_state=42)\n",
        "km.fit(X)\n",
        "print(km.cluster_centers_)\n"
      ],
      "metadata": {
        "id": "OlAdhwkuriIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 27. Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=[1.0, 2.5, 0.5], random_state=42)\n",
        "db = DBSCAN(eps=1.5, min_samples=5).fit(X)\n",
        "labels = db.labels_\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vdVudO64riFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 28. Load the Digits dataset, reduce it to 2D using PCA, and visualize clusters from K-Means\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "digits = load_digits()\n",
        "X = PCA(n_components=2).fit_transform(digits.data)\n",
        "kmeans = KMeans(n_clusters=10, random_state=42).fit(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='tab10')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VAEPSvV1riC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 29. Create synthetic data using make_blobs and evaluate silhouette scores for k = 2 to 5. Display as a bar chart\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X, y = make_blobs(n_samples=500, centers=4, random_state=42)\n",
        "scores = []\n",
        "for k in range(2, 6):\n",
        "    km = KMeans(n_clusters=k, random_state=42).fit(X)\n",
        "    scores.append(silhouette_score(X, km.labels_))\n",
        "\n",
        "plt.bar(range(2, 6), scores)\n",
        "plt.xlabel('Number of clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ser-RDzNriAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30. Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "Z = linkage(load_iris().data, method='average')\n",
        "dendrogram(Z)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4XNF_chrrh9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 31. Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=2.5, random_state=42)\n",
        "km = KMeans(n_clusters=3).fit(X)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "Z = km.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']), alpha=0.4)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=km.labels_, cmap='viridis')\n",
        "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], c='black', marker='X')\n",
        "plt.show()\n",
        "|"
      ],
      "metadata": {
        "id": "HorEPBPPrh6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 32. Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "X = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(load_digits().data)\n",
        "labels = DBSCAN(eps=5, min_samples=5).fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MAxpQaw9rh3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 33. Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result\n",
        "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=3, linkage='complete')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4vsOm7uurh0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 34. Load the Breast Cancer dataset and compare inertia values for K = 2 to 6 using K-Means. Show results in a line plot\n",
        "X = StandardScaler().fit_transform(load_breast_cancer().data)\n",
        "inertias = []\n",
        "for k in range(2, 7):\n",
        "    km = KMeans(n_clusters=k, random_state=42).fit(X)\n",
        "    inertias.append(km.inertia_)\n",
        "\n",
        "plt.plot(range(2, 7), inertias, marker='o')\n",
        "plt.xlabel(\"Number of Clusters (k)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Co_ORMs0rhx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 35. Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage\n",
        "X, y = make_circles(n_samples=300, factor=0.5, noise=0.05)\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='coolwarm')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ig0bfVfgrhvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 36. Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise)\n",
        "X = StandardScaler().fit_transform(load_wine().data)\n",
        "labels = DBSCAN(eps=1.5, min_samples=5).fit_predict(X)\n",
        "\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters (excluding noise):\", n_clusters)\n"
      ],
      "metadata": {
        "id": "HsxQ0OV7rhsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 37. Generate synthetic data with make_blobs and apply KMeans. Then plot the cluster centers on top of the data points\n",
        "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
        "km = KMeans(n_clusters=3, random_state=42).fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=km.labels_, cmap='viridis')\n",
        "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], c='red', marker='X', s=200)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Rz5rsVt4rhqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 38. Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise\n",
        "X = load_iris().data\n",
        "labels = DBSCAN(eps=0.8, min_samples=5).fit_predict(X)\n",
        "n_noise = list(labels).count(-1)\n",
        "print(\"Number of noise samples:\", n_noise)\n"
      ],
      "metadata": {
        "id": "PFT6Wamgrhm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 39. Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result\n",
        "X, y = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
        "km = KMeans(n_clusters=2, random_state=42).fit(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=km.labels_, cmap='coolwarm')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZeHkjxEGrhkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 40. Load the Digits dataset, apply PCA to reduce to 3 components, then use KMeans and visualize with a 3D scatter plot\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "X = PCA(n_components=3).fit_transform(load_digits().data)\n",
        "km = KMeans(n_clusters=10, random_state=42).fit(X)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=km.labels_, cmap='tab10')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mwoKwNQ-rhe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 41. Generate synthetic blobs with 5 centers and apply KMeans. Then use silhouette_score to evaluate the clustering\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X, y = make_blobs(n_samples=500, centers=5, random_state=42)\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "score = silhouette_score(X, labels)\n",
        "print(\"Silhouette Score:\", score)\n"
      ],
      "metadata": {
        "id": "-vOWSR7YrhbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 42. Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = PCA(n_components=2).fit_transform(data.data)\n",
        "agg = AgglomerativeClustering(n_clusters=2)\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Breast Cancer (PCA-reduced)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tyarkHcurhWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 43. Generate noisy circular data using make_circles and visualize clustering results from KMeans and DBSCAN side-by-side\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_circles(n_samples=500, factor=0.5, noise=0.05)\n",
        "kmeans = KMeans(n_clusters=2, random_state=42).fit(X)\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5).fit(X)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='coolwarm')\n",
        "ax1.set_title('KMeans')\n",
        "ax2.scatter(X[:, 0], X[:, 1], c=dbscan.labels_, cmap='coolwarm')\n",
        "ax2.set_title('DBSCAN')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "17LQFzMNrhTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 44. Load the Iris dataset and plot the Silhouette Coefficient for each sample after KMeans clustering\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "kmeans = KMeans(n_clusters=3, random_state=42).fit(X)\n",
        "labels = kmeans.labels_\n",
        "silhouette_vals = silhouette_samples(X, labels)\n",
        "\n",
        "plt.bar(range(len(X)), silhouette_vals, color='teal')\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Silhouette Coefficient')\n",
        "plt.title('Silhouette Coefficient per sample - Iris')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IA-OCoEErhQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 45. Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters\n",
        "X, y = make_blobs(n_samples=400, centers=4, random_state=42)\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10')\n",
        "plt.title(\"Agglomerative Clustering (average linkage)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nJc2ra7grhFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 46. Load the Wine dataset, apply KMeans, and visualize the cluster assignments in a seaborn pairplot (first 4 features)\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine()\n",
        "X = pd.DataFrame(wine.data[:, :4], columns=wine.feature_names[:4])\n",
        "labels = KMeans(n_clusters=3, random_state=42).fit_predict(wine.data)\n",
        "\n",
        "X['cluster'] = labels\n",
        "sns.pairplot(X, hue='cluster', palette='tab10')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H-hAP5pHs1lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 47. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count\n",
        "X, y = make_blobs(n_samples=500, centers=4, cluster_std=1.5, random_state=42)\n",
        "db = DBSCAN(eps=1.2, min_samples=5).fit(X)\n",
        "labels = db.labels_\n",
        "\n",
        "import numpy as np\n",
        "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = list(labels).count(-1)\n",
        "\n",
        "print(\"Clusters found:\", n_clusters)\n",
        "print(\"Noise points:\", n_noise)\n"
      ],
      "metadata": {
        "id": "0BS0bOKKs1a8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 48. Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "digits = load_digits()\n",
        "X_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(digits.data)\n",
        "agg = AgglomerativeClustering(n_clusters=10).fit(X_tsne)\n",
        "\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=agg.labels_, cmap='tab10')\n",
        "plt.title(\"Agglomerative Clustering on Digits (t-SNE reduced)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X70ag9bTs1TN"
      }
    }
  ]
}